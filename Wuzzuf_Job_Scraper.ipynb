{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "search_term = \"Data Analyst\"\n",
        "pages_to_scrape = 5\n",
        "output_file = 'wuzzuf_data_analyst_jobs_v4.csv'\n",
        "\n",
        "# Anti-Blocking Headers\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "\n",
        "data_list = []\n",
        "print(f\"üöÄ Starting SMART Scraper V4 for '{search_term}'...\")\n",
        "\n",
        "for page in range(pages_to_scrape):\n",
        "    url = f\"https://wuzzuf.net/search/jobs/?a=hpb&q={search_term.replace(' ', '%20')}&start={page}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # 1. Find all Job Titles (H2 tags are usually stable)\n",
        "        titles = soup.find_all('h2')\n",
        "\n",
        "        print(f\"üìÑ Page {page}: Processing {len(titles)} jobs...\")\n",
        "\n",
        "        for h2 in titles:\n",
        "            try:\n",
        "                # --- SMART EXTRACTION STRATEGY ---\n",
        "\n",
        "                # 1. Title & Link\n",
        "                link_tag = h2.find('a')\n",
        "                if not link_tag: continue\n",
        "\n",
        "                title = link_tag.get_text(strip=True)\n",
        "                link = link_tag['href']\n",
        "                if not link.startswith('http'):\n",
        "                    link = \"https://wuzzuf.net\" + link  # Fix relative links\n",
        "\n",
        "                # 2. Get the Container (The Card)\n",
        "                # We go up to find the parent container that holds everything\n",
        "                # Usually h2 -> div -> div (Card)\n",
        "                card = h2.find_parent('div')\n",
        "                if card:\n",
        "                    card = card.find_parent('div') # Go one level higher just in case\n",
        "\n",
        "                if not card: continue\n",
        "\n",
        "                # 3. Company (Usually the 2nd link in the card, after the title)\n",
        "                all_links = card.find_all('a')\n",
        "                if len(all_links) > 1:\n",
        "                    company = all_links[1].get_text(strip=True).replace('-', '').strip()\n",
        "                else:\n",
        "                    company = \"Confidential\"\n",
        "\n",
        "                # 4. Location (Look for span tags)\n",
        "                spans = card.find_all('span')\n",
        "                if len(spans) > 0:\n",
        "                    location = spans[0].get_text(strip=True)\n",
        "                else:\n",
        "                    location = \"Unknown\"\n",
        "\n",
        "                # 5. Skills (Get text from the bottom area)\n",
        "                # We look for the last div or just collect text that looks like skills\n",
        "                # Heuristic: Skills are usually separated by dots or newlines at the end\n",
        "                card_text = card.get_text(separator='|', strip=True)\n",
        "                # Split by '|' and take the last few parts as skills roughly\n",
        "                skills_rough = card_text.split('|')[-4:]\n",
        "                skills = \", \".join(skills_rough).replace('Apply', '').strip()\n",
        "\n",
        "                data_list.append({\n",
        "                    'Job Title': title,\n",
        "                    'Company Name': company,\n",
        "                    'Location': location,\n",
        "                    'Skills/Description': skills,\n",
        "                    'Job Link': link\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        time.sleep(1) # Be nice to the server\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error on page {page}: {e}\")\n",
        "\n",
        "# --- SAVE & CHECK ---\n",
        "df = pd.DataFrame(data_list)\n",
        "print(\"-\" * 30)\n",
        "print(f\"‚úÖ DONE! Total Jobs: {len(df)}\")\n",
        "# Show a sample to check if columns are filled\n",
        "print(df[['Job Title', 'Company Name', 'Location']].head())\n",
        "\n",
        "df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "6CEHNTlfTOzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load the Data\n",
        "try:\n",
        "    df = pd.read_csv('wuzzuf_data_analyst_jobs_v4.csv')\n",
        "    print(f\"‚úÖ Data Loaded Successfully: {len(df)} jobs.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: File not found. Please upload 'wuzzuf_data_analyst_jobs_v4.csv'.\")\n",
        "\n",
        "# --- DATA PROCESSING ---\n",
        "\n",
        "# A. Experience Level (Derived from Job Title)\n",
        "def get_experience(title):\n",
        "    title = str(title).lower()\n",
        "    if 'senior' in title: return 'Senior'\n",
        "    elif 'junior' in title or 'entry' in title: return 'Junior/Entry'\n",
        "    elif 'manager' in title or 'lead' in title or 'head' in title: return 'Manager/Lead'\n",
        "    else: return 'Mid-Level'\n",
        "\n",
        "df['Experience_Level'] = df['Job Title'].apply(get_experience)\n",
        "\n",
        "# B. Location Cleaning\n",
        "# Take the first part of the location (e.g., \"New Cairo, Cairo\" -> \"New Cairo\")\n",
        "df['Clean_Location'] = df['Location'].astype(str).apply(lambda x: x.split(',')[0].strip())\n",
        "\n",
        "# C. Skills Extraction (The Magic Part ‚ú®)\n",
        "# We will search for these keywords in the 'Skills/Description' column\n",
        "target_skills = ['Python', 'SQL', 'Excel', 'Power BI', 'Tableau',\n",
        "                 'Machine Learning', 'R', 'Big Data', 'Spark', 'AWS', 'Azure',\n",
        "                 'Data Modeling', 'Statistics', 'Visualization']\n",
        "\n",
        "found_skills = []\n",
        "for text in df['Skills/Description'].fillna('').astype(str):\n",
        "    for skill in target_skills:\n",
        "        # Check if the skill exists in the text (case insensitive)\n",
        "        if skill.lower() in text.lower():\n",
        "            found_skills.append(skill)\n",
        "\n",
        "# Create a DataFrame for Skills\n",
        "skills_counts = pd.DataFrame(Counter(found_skills).most_common(10), columns=['Skill', 'Count'])\n",
        "\n",
        "# --- VISUALIZATION (Dark Theme for Professional Look) ---\n",
        "\n",
        "# Chart 1: Top 10 In-Demand Skills (Bar Chart)\n",
        "fig1 = px.bar(skills_counts.sort_values('Count', ascending=True),\n",
        "              x='Count', y='Skill', orientation='h',\n",
        "              title='üî• Top 10 In-Demand Data Analyst Skills in Egypt',\n",
        "              text='Count', color='Count', template='plotly_dark')\n",
        "fig1.update_layout(xaxis_title=\"Number of Jobs\", yaxis_title=\"Skill\")\n",
        "fig1.show()\n",
        "\n",
        "# Chart 2: Experience Level Required (Donut Chart)\n",
        "exp_counts = df['Experience_Level'].value_counts().reset_index()\n",
        "exp_counts.columns = ['Level', 'Count']\n",
        "fig2 = px.pie(exp_counts, values='Count', names='Level', hole=0.5,\n",
        "              title='üéì Job Market by Experience Level',\n",
        "              color_discrete_sequence=px.colors.sequential.RdBu,\n",
        "              template='plotly_dark')\n",
        "fig2.show()\n",
        "\n",
        "# Chart 3: Top Locations (Bar Chart)\n",
        "loc_counts = df['Clean_Location'].value_counts().head(7).reset_index()\n",
        "loc_counts.columns = ['Location', 'Count']\n",
        "fig3 = px.bar(loc_counts, x='Location', y='Count',\n",
        "              title='üìç Where are the Jobs Located?',\n",
        "              color='Count', template='plotly_dark')\n",
        "fig3.show()"
      ],
      "metadata": {
        "id": "BY_kcp1ST_nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load the V4 Data\n",
        "try:\n",
        "    df = pd.read_csv('wuzzuf_data_analyst_jobs_v4.csv')\n",
        "    print(f\"‚úÖ Data Loaded: {len(df)} jobs.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: File not found. Run the V4 scraper first.\")\n",
        "\n",
        "# --- DATA CLEANING ---\n",
        "df['Clean_Location'] = df['Location'].astype(str).apply(lambda x: x.split(',')[0].strip())\n",
        "\n",
        "# Experience Logic\n",
        "def get_exp(title):\n",
        "    t = str(title).lower()\n",
        "    if 'senior' in t: return 'Senior'\n",
        "    elif 'junior' in t or 'entry' in t: return 'Junior/Entry'\n",
        "    elif 'manager' in t or 'lead' in t: return 'Manager/Lead'\n",
        "    return 'Mid-Level'\n",
        "df['Experience'] = df['Job Title'].apply(get_exp)\n",
        "\n",
        "# --- SKILLS EXTRACTION ---\n",
        "# Keywords we care about\n",
        "keywords = ['Python', 'SQL', 'Excel', 'Power BI', 'Tableau', 'Machine Learning',\n",
        "            'R', 'Big Data', 'Spark', 'AWS', 'Azure', 'NoSQL']\n",
        "\n",
        "all_skills = []\n",
        "for text in df['Skills/Description'].fillna('').astype(str):\n",
        "    for word in keywords:\n",
        "        if word.lower() in text.lower():\n",
        "            all_skills.append(word)\n",
        "\n",
        "# --- VISUALIZATION ---\n",
        "if all_skills:\n",
        "    # 1. Top Skills\n",
        "    skills_df = pd.DataFrame(Counter(all_skills).most_common(10), columns=['Skill', 'Count'])\n",
        "    fig1 = px.bar(skills_df, x='Count', y='Skill', orientation='h',\n",
        "                  title='üî• Top Data Analyst Skills in Egypt',\n",
        "                  text='Count', color='Count', template='plotly_dark')\n",
        "    fig1.show()\n",
        "\n",
        "    # 2. Experience Level\n",
        "    fig2 = px.pie(df, names='Experience', title='üéì Experience Level Required',\n",
        "                  hole=0.4, template='plotly_dark')\n",
        "    fig2.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No skills found. The scraper might need adjustment.\")"
      ],
      "metadata": {
        "id": "15mt_AENUL5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}